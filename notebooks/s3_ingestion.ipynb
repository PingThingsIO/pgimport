{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingest Data From s3 to BTrDB\n",
    "This notebook will demonstrate how to subclass `CloudMixin` to create a `DataParser` that can load data from AWS s3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import s3fs\n",
    "import btrdb\n",
    "import pandas as pd\n",
    "\n",
    "from pgimport.csv_parser import CSVParser\n",
    "from pgimport.ingest import DataIngestor\n",
    "from pgimport.cloud import S3Mixin, S3File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Custom File\n",
    "We can extend the `File` object to include a `count`, which can be passed to a `DataIngestor` and allows for a progress bar during ingestion.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class S3ProgBarFile(S3File):\n",
    "    def __init__(self, path, count=True, header=False):\n",
    "        super().__init__(path)\n",
    "        # TODO: this works but its slow, need to find a more efficient way to do this\n",
    "        if count:\n",
    "            header_rows = 1 if header else 0\n",
    "            df = pd.read_csv(path)\n",
    "            self.count = (len(df) - header_rows) * (len(df.columns) - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Custom DataParser\n",
    "We can create a custom `DataParser` by extending the `CSVParser` and combining it with the `S3Mixin`, which provides a connection to s3 by assigning a `client` attribute that is equivalent to `boto3.client(\"s3\")`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class S3CSVParser(S3Mixin, CSVParser):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    -----------\n",
    "    collection_prefix: str\n",
    "        prefix to add to all streams' collection names\n",
    "    regex: str\n",
    "        regex string to use to retrieve collection from file name\n",
    "    metadata: dict or str\n",
    "        either a dict of metadata or a str filename referring to a yaml/json metadata file\n",
    "    meta_func:\n",
    "        callback function to use to map metadata to Stream objects\n",
    "    bucket: str\n",
    "        s3 bucket that contains raw data files\n",
    "    s3_prefix: str\n",
    "        (optional) subdirectory within bucket that contains raw data files \n",
    "    **kwargs: dict\n",
    "        (optional) key/value pairs specifying required AWS credentials. Can be left as None\n",
    "        if credentials are stored as environtment variables\n",
    "    \"\"\"\n",
    "    def __init__(self, collection_prefix=None, regex=None, metadata=None, meta_func=None, bucket=None, s3_prefix=None, **kwargs):\n",
    "        # init CSVParser\n",
    "        CSVParser.__init__(self, collection_prefix=collection_prefix, regex=regex, metadata=metadata, meta_func=meta_func)\n",
    "        # establish attrs that are specific to this class\n",
    "        self.bucket = bucket\n",
    "        self.s3_prefix = s3_prefix\n",
    "        # init S3Mixin, connects to s3 and provides a handle via self.client\n",
    "        S3Mixin.__init__(self, **kwargs)\n",
    "        self.client = self.connect()\n",
    "    \n",
    "    def collect_files(self):\n",
    "        return [S3ProgBarFile(fpath, header=True) for fpath in self.list_objects(self.bucket, prefix=self.s3_prefix)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingest s3 Data \n",
    "Next we will use the `S3CSVParser` to load data from s3, create `Stream` objects, and pass them to the `DataIngestor` for insertion into BTrDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 files\n",
      "Files contain 6480000 points\n"
     ]
    }
   ],
   "source": [
    "# bucket = \"your-bucket\"\n",
    "# subdir = \"any-directory-within-bucket\"\n",
    "bucket = \"pt-ni4ai-atc\"\n",
    "prefix = \"test_csvs\"\n",
    "\n",
    "# explicity setting credentials is optional, you can also just store them as env vars\n",
    "aws_creds = {\n",
    "    \"aws_access_key_id\": os.environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "    \"aws_secret_access_key\": os.environ[\"AWS_SECRET_ACCESS_KEY\"],\n",
    "    \"aws_session_token\": os.environ[\"AWS_SESSION_TOKEN\"]\n",
    "}\n",
    "\n",
    "s3 = S3CSVParser(collection_prefix=\"test_ingest\", bucket=bucket, s3_prefix=prefix, **aws_creds)\n",
    "files = s3.collect_files()\n",
    "total_points = sum([f.count for f in files])\n",
    "\n",
    "print(f\"Found {len(files)} files\")\n",
    "print(f\"Files contain {total_points} points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6480060it [05:40, 24656.75it/s]                             "
     ]
    }
   ],
   "source": [
    "# replace with your BTrDB credentials\n",
    "conn = btrdb.connect(os.environ[\"BTRDB_ENDPOINTS\"], apikey=os.environ[\"BTRDB_API_KEY\"])\n",
    "\n",
    "ingestor = DataIngestor(conn, total_points=total_points)\n",
    "for streams in s3.create_streams(files):\n",
    "    ingestor.ingest(streams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
